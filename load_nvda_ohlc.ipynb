{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# NVDA OHLC Data Exploratory Data Analysis\n",
        "\n",
        "This notebook loads all OHLC CSV files from the `data/` folder into a single DataFrame and performs exploratory data analysis following the EDA plan.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available CSV files:\n",
            "Found 1107 CSV files\n",
            "data/20210104_ohlc_NVDA.csv\n",
            "data/20210105_ohlc_NVDA.csv\n",
            "data/20210106_ohlc_NVDA.csv\n",
            "data/20210107_ohlc_NVDA.csv\n",
            "data/20210108_ohlc_NVDA.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Available CSV files:\")\n",
        "# Path to data folder\n",
        "data_path = 'data'\n",
        "\n",
        "# Find all relevant CSV files\n",
        "csv_files = glob.glob(os.path.join(data_path, '*_ohlc_NVDA.csv'))\n",
        "print(f\"Found {len(csv_files)} CSV files\")\n",
        "\n",
        "# Sort files by date to ensure consistent ordering\n",
        "csv_files.sort()\n",
        "\n",
        "for file in csv_files[:5]:  # Show first 5 files\n",
        "    print(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1107 files\n",
            "Combined DataFrame shape: (865782, 12)\n",
            "Date range: 2021-01-04 to 2025-05-30\n",
            "Columns: ['time', 'timestamp', 'open', 'high', 'low', 'close', 'vwap', 'volume', 'transactions', 'otc', 'date', 'date_str']\n"
          ]
        }
      ],
      "source": [
        "# Load and combine all CSV files\n",
        "dfs = []\n",
        "\n",
        "for file in csv_files:\n",
        "    # Extract date from filename (first 8 digits: YYYYMMDD)\n",
        "    basename = os.path.basename(file)\n",
        "    date_str = basename.split('_')[0]  # e.g., '20250529' from '20250529_ohlc_NVDA.csv'\n",
        "    \n",
        "    # Convert to proper date format\n",
        "    date_obj = datetime.strptime(date_str, '%Y%m%d').date()\n",
        "    \n",
        "    # Read CSV\n",
        "    df = pd.read_csv(file)\n",
        "    \n",
        "    # Add date column\n",
        "    df['date'] = date_obj\n",
        "    df['date_str'] = date_str\n",
        "    \n",
        "    dfs.append(df)\n",
        "    \n",
        "print(f\"Loaded {len(dfs)} files\")\n",
        "\n",
        "# Concatenate all DataFrames\n",
        "all_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(f\"Combined DataFrame shape: {all_data.shape}\")\n",
        "print(f\"Date range: {all_data['date'].min()} to {all_data['date'].max()}\")\n",
        "print(f\"Columns: {list(all_data.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 rows:\n",
            "                time      timestamp     open     high      low    close  \\\n",
            "0  20210104 04:07:00  1609751220000  13.0800  13.0800  13.0700  13.0700   \n",
            "1  20210104 04:16:00  1609751760000  13.1200  13.1245  13.1200  13.1245   \n",
            "2  20210104 04:17:00  1609751820000  13.1223  13.1223  13.1223  13.1223   \n",
            "3  20210104 04:24:00  1609752240000  13.1250  13.1250  13.1250  13.1250   \n",
            "4  20210104 04:59:00  1609754340000  13.1568  13.1568  13.1568  13.1568   \n",
            "5  20210104 05:14:00  1609755240000  13.1500  13.1500  13.1500  13.1500   \n",
            "6  20210104 05:19:00  1609755540000  13.1380  13.1380  13.1380  13.1380   \n",
            "7  20210104 05:21:00  1609755660000  13.1380  13.1380  13.1380  13.1380   \n",
            "8  20210104 05:27:00  1609756020000  13.1375  13.1375  13.1375  13.1375   \n",
            "9  20210104 05:58:00  1609757880000  13.1375  13.1375  13.1375  13.1375   \n",
            "\n",
            "      vwap   volume  transactions  otc        date  date_str  \n",
            "0  13.0792  21840.0            11  NaN  2021-01-04  20210104  \n",
            "1  13.1130  45440.0            26  NaN  2021-01-04  20210104  \n",
            "2  13.1121   5440.0             2  NaN  2021-01-04  20210104  \n",
            "3  13.1250  12000.0             3  NaN  2021-01-04  20210104  \n",
            "4  13.1550  22840.0            16  NaN  2021-01-04  20210104  \n",
            "5  13.1493   8040.0             8  NaN  2021-01-04  20210104  \n",
            "6  13.1383   7560.0             7  NaN  2021-01-04  20210104  \n",
            "7  13.1378  20240.0             9  NaN  2021-01-04  20210104  \n",
            "8  13.1375  13840.0             8  NaN  2021-01-04  20210104  \n",
            "9  13.1375   9920.0             6  NaN  2021-01-04  20210104  \n",
            "\n",
            "Data types:\n",
            "time             object\n",
            "timestamp         int64\n",
            "open            float64\n",
            "high            float64\n",
            "low             float64\n",
            "close           float64\n",
            "vwap            float64\n",
            "volume          float64\n",
            "transactions      int64\n",
            "otc             float64\n",
            "date             object\n",
            "date_str         object\n",
            "dtype: object\n",
            "\n",
            "Sample of unique dates:\n",
            "[datetime.date(2021, 1, 4), datetime.date(2021, 1, 5), datetime.date(2021, 1, 6), datetime.date(2021, 1, 7), datetime.date(2021, 1, 8), datetime.date(2021, 1, 11), datetime.date(2021, 1, 12), datetime.date(2021, 1, 13), datetime.date(2021, 1, 14), datetime.date(2021, 1, 15)]\n"
          ]
        }
      ],
      "source": [
        "# Display sample data\n",
        "print(\"First 10 rows:\")\n",
        "print(all_data.head(10))\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(all_data.dtypes)\n",
        "\n",
        "print(\"\\nSample of unique dates:\")\n",
        "print(sorted(all_data['date'].unique())[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Summary:\n",
            "Total records: 865,782\n",
            "Date range: 2021-01-04 to 2025-05-30\n",
            "Number of unique dates: 1107\n",
            "Average records per day: 782.1\n",
            "\n",
            "Missing values per column:\n",
            "time                 0\n",
            "timestamp            0\n",
            "open                 0\n",
            "high                 0\n",
            "low                  0\n",
            "close                0\n",
            "vwap                 0\n",
            "volume               0\n",
            "transactions         0\n",
            "otc             865782\n",
            "date                 0\n",
            "date_str             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Optional: Save the combined dataset\n",
        "# all_data.to_csv('combined_nvda_ohlc.csv', index=False)\n",
        "# print(\"Combined data saved to 'combined_nvda_ohlc.csv'\")\n",
        "\n",
        "# Data summary\n",
        "print(\"Data Summary:\")\n",
        "print(f\"Total records: {len(all_data):,}\")\n",
        "print(f\"Date range: {all_data['date'].min()} to {all_data['date'].max()}\")\n",
        "print(f\"Number of unique dates: {all_data['date'].nunique()}\")\n",
        "print(f\"Average records per day: {len(all_data) / all_data['date'].nunique():.1f}\")\n",
        "\n",
        "# Check for any missing data\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(all_data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Cleaning\n",
        "\n",
        "Let's ensure our data types are correct and check for duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 0\n",
            "\n",
            "Updated data types:\n",
            "time                    object\n",
            "timestamp                int64\n",
            "open                   float64\n",
            "high                   float64\n",
            "low                    float64\n",
            "close                  float64\n",
            "vwap                   float64\n",
            "volume                 float64\n",
            "transactions             int64\n",
            "otc                    float64\n",
            "date            datetime64[ns]\n",
            "date_str                object\n",
            "datetime        datetime64[ns]\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Convert date column to datetime type\n",
        "all_data['date'] = pd.to_datetime(all_data['date'])\n",
        "\n",
        "# Create a proper datetime column by combining date and time\n",
        "all_data['datetime'] = pd.to_datetime(all_data['date'].dt.strftime('%Y-%m-%d') + ' ' + all_data['time'].str.split(' ').str[1])\n",
        "\n",
        "# Check for duplicate rows\n",
        "duplicates = all_data.duplicated()\n",
        "print(f\"Number of duplicate rows: {duplicates.sum()}\")\n",
        "\n",
        "# Drop duplicates if any\n",
        "if duplicates.sum() > 0:\n",
        "    all_data = all_data.drop_duplicates()\n",
        "    print(f\"Dropped {duplicates.sum()} duplicate rows\")\n",
        "    print(f\"New DataFrame shape: {all_data.shape}\")\n",
        "\n",
        "# Display the updated data types\n",
        "print(\"\\nUpdated data types:\")\n",
        "print(all_data.dtypes)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Quality Checks\n",
        "\n",
        "Let's check for any data quality issues in price and volume columns, and identify gaps in the time series.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for zero or negative values in price/volume columns\n",
        "price_cols = ['open', 'high', 'low', 'close', 'vwap']\n",
        "volume_cols = ['volume', 'transactions']\n",
        "\n",
        "print(\"Zero or negative values in price columns:\")\n",
        "for col in price_cols:\n",
        "    zero_or_neg = (all_data[col] <= 0).sum()\n",
        "    print(f\"{col}: {zero_or_neg} zero or negative values\")\n",
        "\n",
        "print(\"\\nZero or negative values in volume columns:\")\n",
        "for col in volume_cols:\n",
        "    zero_or_neg = (all_data[col] <= 0).sum()\n",
        "    print(f\"{col}: {zero_or_neg} zero or negative values\")\n",
        "\n",
        "# Check if open, high, low, close values are consistent\n",
        "print(\"\\nInconsistent OHLC values:\")\n",
        "inconsistent_high_low = (all_data['high'] < all_data['low']).sum()\n",
        "print(f\"High < Low: {inconsistent_high_low} occurrences\")\n",
        "\n",
        "inconsistent_open_high = (all_data['open'] > all_data['high']).sum()\n",
        "print(f\"Open > High: {inconsistent_open_high} occurrences\")\n",
        "\n",
        "inconsistent_open_low = (all_data['open'] < all_data['low']).sum()\n",
        "print(f\"Open < Low: {inconsistent_open_low} occurrences\")\n",
        "\n",
        "inconsistent_close_high = (all_data['close'] > all_data['high']).sum()\n",
        "print(f\"Close > High: {inconsistent_close_high} occurrences\")\n",
        "\n",
        "inconsistent_close_low = (all_data['close'] < all_data['low']).sum()\n",
        "print(f\"Close < Low: {inconsistent_close_low} occurrences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Investigate gaps in the time series\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter\n",
        "\n",
        "# Count records per day\n",
        "records_per_day = all_data.groupby('date').size()\n",
        "mean_records = records_per_day.mean()\n",
        "std_records = records_per_day.std()\n",
        "\n",
        "print(f\"Average records per day: {mean_records:.1f}\")\n",
        "print(f\"Standard deviation of records per day: {std_records:.1f}\")\n",
        "\n",
        "# Identify days with significantly fewer records (potential gaps)\n",
        "threshold = mean_records - 2 * std_records\n",
        "days_with_gaps = records_per_day[records_per_day < threshold]\n",
        "\n",
        "if len(days_with_gaps) > 0:\n",
        "    print(f\"\\nDays with significantly fewer records (potential gaps):\")\n",
        "    print(days_with_gaps.sort_values())\n",
        "\n",
        "# Plot records per day\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(records_per_day.index, records_per_day.values)\n",
        "plt.axhline(y=mean_records, color='r', linestyle='-', alpha=0.3, label=f'Mean: {mean_records:.1f}')\n",
        "plt.axhline(y=threshold, color='r', linestyle='--', alpha=0.3, \n",
        "            label=f'Threshold for gaps: {threshold:.1f}')\n",
        "plt.title('Number of Records per Day')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Records')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Time Coverage & Granularity\n",
        "\n",
        "Let's analyze the date range, trading hours, and granularity of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the hour and minute from the time column\n",
        "all_data['hour'] = all_data['datetime'].dt.hour\n",
        "all_data['minute'] = all_data['datetime'].dt.minute\n",
        "all_data['day_of_week'] = all_data['datetime'].dt.day_name()\n",
        "\n",
        "# Define regular and extended trading hours\n",
        "regular_hours = (all_data['hour'] >= 9) & (all_data['hour'] < 16) | \\\n",
        "                ((all_data['hour'] == 16) & (all_data['minute'] == 0))\n",
        "extended_hours = ~regular_hours\n",
        "\n",
        "# Count records by trading hours type\n",
        "print(\"Trading Hours Distribution:\")\n",
        "print(f\"Regular trading hours (9:30 AM - 4:00 PM): {regular_hours.sum():,} records ({regular_hours.mean()*100:.1f}%)\")\n",
        "print(f\"Extended trading hours: {extended_hours.sum():,} records ({extended_hours.mean()*100:.1f}%)\")\n",
        "\n",
        "# Analyze trading days of the week\n",
        "day_counts = all_data['day_of_week'].value_counts().sort_index()\n",
        "print(\"\\nTrading Days Distribution:\")\n",
        "for day, count in day_counts.items():\n",
        "    print(f\"{day}: {count:,} records ({count/len(all_data)*100:.1f}%)\")\n",
        "\n",
        "# Visualize distribution of records by hour of day\n",
        "hour_counts = all_data.groupby('hour').size()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(hour_counts.index, hour_counts.values)\n",
        "plt.title('Distribution of Records by Hour of Day')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Records')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(True, axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Analyze time granularity (minute intervals)\n",
        "time_diffs = all_data.sort_values('datetime').groupby('date')['datetime'].diff().dt.seconds / 60\n",
        "time_diffs = time_diffs.dropna()\n",
        "\n",
        "# Get the most common time differences\n",
        "time_diff_counts = time_diffs.value_counts().sort_index()\n",
        "print(\"\\nMinute intervals between records:\")\n",
        "for interval, count in time_diff_counts.head(5).items():\n",
        "    print(f\"{interval} minute interval: {count:,} occurrences ({count/len(time_diffs)*100:.1f}%)\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(time_diffs, bins=30, range=(0, 30))\n",
        "plt.title('Distribution of Time Intervals Between Records')\n",
        "plt.xlabel('Minutes Between Records')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
